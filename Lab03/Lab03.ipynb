{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcFhSB8lde8B"
      },
      "source": [
        "# Lab 03: Embedded Deep Learning\n",
        "\n",
        "In this lab session, we will optimize the deep learning model that was trained in the last session. Later we will put the quantized model to the ESP32 MCU.\n",
        "\n",
        "![ESP-EYE](https://media.digikey.com/Photos/Espressif%20Systems/MFG_ESP-EYE_Top.jpg)\n",
        "\n",
        "The first half session can be run on google colab. Open in google colab -> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg#left)](https://colab.research.google.com/github/SuperChange001/deeplearning_labs/blob/main/Lab03/Lab03.ipynb)\n",
        "\n",
        "But the latter part must be running on your PC, below are tutorials of installing required softwares for your PC:\n",
        "1. [VS Code Editor](https://code.visualstudio.com/download)\n",
        "2. [Git Client](https://git-scm.com/downloads/guis)\n",
        "3. [ESP-IDF](https://github.com/espressif/vscode-esp-idf-extension/blob/master/docs/tutorial/install.md)\n",
        "\n",
        "*Tips:* It is recommended to try to install all the software before you come to the Lab. But, don't worry if you fail to install everything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EV4SBCecxWZ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP98q8pBb0vc"
      },
      "source": [
        "First, we import some libraries for image processing and utils, as well as TensorFlow. Note that the module `image_dataset_from_directory` is necessary for downloading our data set from Google."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpyxHufwFKYp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 32\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPfEkbUYG_ci"
      },
      "source": [
        "## Import the Gesture dataset for evaluating the pretrianed model\n",
        "\n",
        "Download and extract the `zip` file containing the datasets with `tf.keras.utils.get_file`. \n",
        "\n",
        "*Tips: change the code respectively if you have a model for other task.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy8hBBOCMlA0"
      },
      "outputs": [],
      "source": [
        "# Download our dataset used for training\n",
        "TRAIN_SET_URL = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('rps.zip', origin=TRAIN_SET_URL, extract=True, cache_dir='/content')\n",
        "train_dir = os.path.join(os.path.dirname(path_to_zip), \"rps\")\n",
        "\n",
        "# As well as the validation dataset\n",
        "VAL_SET_URL = 'https://storage.googleapis.com/laurencemoroney-blog.appspot.com/rps-test-set.zip'\n",
        "path_to_zip2 = tf.keras.utils.get_file('rps-test-set.zip', origin=VAL_SET_URL, extract=True, cache_dir='/content')\n",
        "validation_dir = os.path.join(os.path.dirname(path_to_zip2), \"rps-test-set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HBernrNHwFc"
      },
      "source": [
        "Then we can generate tf.data.Dataset from image files in a directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0HnHBGmNLh5",
        "outputId": "355a75dd-9b80-4bd2-ddae-5701f281ded2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 2520 files belonging to 3 classes.\n",
            "Found 372 files belonging to 3 classes.\n"
          ]
        }
      ],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (96, 96) \n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             image_size=IMG_SIZE)\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(validation_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=BATCH_SIZE,\n",
        "                                                  image_size=IMG_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRHrIWLUJDQR"
      },
      "source": [
        "Lets display some images of our dataset, as well as the class names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGJ5uDKeJuvN"
      },
      "source": [
        "### Split test set and validation set\n",
        "We are now taking a fifth of the validation dataset to use as our test set. The validation set will be used for observing if we got overfitting during training while the test set is for the final test after training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkbBixY8NVbP",
        "outputId": "02077a02-78d4-497a-9970-b6bf233e1276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of validation batches: 10\n",
            "Number of test batches: 2\n"
          ]
        }
      ],
      "source": [
        "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "\n",
        "test_dataset = validation_dataset.take(val_batches // 5)\n",
        "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
        "\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSjEqLOIspiZ"
      },
      "source": [
        "## Upload model and load the model from hard disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXUFpmD03n_v"
      },
      "source": [
        "Fetch the example model from github, you can also upload your own model to colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY9rzyf5t1gj"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import h5py\n",
        "import requests\n",
        "\n",
        "url = 'https://github.com/SuperChange001/deeplearning_labs/raw/main/Lab03/pretrained_models/model_rps.h5'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "\n",
        "with open('/content/model_rps.h5', 'wb') as f:\n",
        "    f.write(r.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAA6tMyZ3n_w"
      },
      "source": [
        "Alternative way for fetching the pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP_iYinhrShE",
        "outputId": "693486ec-fd83-465e-ee92-94fd68b6bdab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-08-15 14:53:04--  https://github.com/SuperChange001/deeplearning_labs/raw/main/Lab03/pretrained_models/model_rps.h5\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/SuperChange001/deeplearning_labs/main/Lab03/pretrained_models/model_rps.h5 [following]\n",
            "--2022-08-15 14:53:04--  https://raw.githubusercontent.com/SuperChange001/deeplearning_labs/main/Lab03/pretrained_models/model_rps.h5\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3441936 (3.3M) [application/octet-stream]\n",
            "Saving to: ‘model_rps.h5’\n",
            "\n",
            "\rmodel_rps.h5          0%[                    ]       0  --.-KB/s               \rmodel_rps.h5        100%[===================>]   3.28M  --.-KB/s    in 0.01s   \n",
            "\n",
            "2022-08-15 14:53:04 (291 MB/s) - ‘model_rps.h5’ saved [3441936/3441936]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/SuperChange001/deeplearning_labs/raw/main/Lab03/pretrained_models/model_rps.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22HmS5zM3n_w"
      },
      "source": [
        "Load model from hard disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wzU3N-MsqeQ",
        "outputId": "350e7ed7-38e2-4330-caef-4a7e5f47980a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
            "                                                                 \n",
            " tf.math.truediv (TFOpLambda  (None, 96, 96, 3)        0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " tf.math.subtract (TFOpLambd  (None, 96, 96, 3)        0         \n",
            " a)                                                              \n",
            "                                                                 \n",
            " mobilenetv2_0.35_96 (Functi  (None, 3, 3, 1280)       410208    \n",
            " onal)                                                           \n",
            "                                                                 \n",
            " global_average_pooling2d (G  (None, 1280)             0         \n",
            " lobalAveragePooling2D)                                          \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 1280)              0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 3)                 3843      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 414,051\n",
            "Trainable params: 337,699\n",
            "Non-trainable params: 76,352\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = tf.keras.models.load_model('/content/model_rps.h5')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yeFoHyg3n_x"
      },
      "source": [
        "Test Accuracy of the loaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1OfCsJHwAx0",
        "outputId": "0419e4e9-e856-4d4a-a2a2-1b3e376050ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2/2 [==============================] - 1s 69ms/step - loss: 0.2492 - accuracy: 0.9219\n",
            "Test accuracy : 0.921875\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print('Test accuracy :', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFWEgNO83n_x"
      },
      "source": [
        "## Convert model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZbiMdnR03n_x"
      },
      "outputs": [],
      "source": [
        "# Parameters setting\n",
        "optimization_config = [tf.lite.Optimize.DEFAULT]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI6M0VxC3n_y"
      },
      "source": [
        "### Convert to a TensorFlow Lite model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJA1ep4S3n_y",
        "outputId": "28de8cfb-5a4c-4209-c8d5-28ae6a1c6fb8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "#TF Lite model without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJtFP0rd3n_y"
      },
      "source": [
        "### Convert using dynamic range quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlYVUayf3n_y",
        "outputId": "0c08b8e2-cdf6-4e06-d2ef-b0a558aa33ec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "#TF Lite model with dynamic range quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = optimization_config\n",
        "\n",
        "tflite_model_dynamic_range = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byb0_jnX3n_y"
      },
      "source": [
        "### Convert using float fallback quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHI_tjr93n_y",
        "outputId": "2052439c-5ed3-4bcf-c467-2e35c0186aec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "#Extracts sample images needed for float fallback and full integer quantization \n",
        "def representative_data_gen():\n",
        "  for input in train_dataset.take(4):\n",
        "    for input_value in tf.data.Dataset.from_tensor_slices(np.array(input[0])).batch(1).take(32):\n",
        "      yield [input_value]\n",
        "\n",
        "#TF Lite model with Float Fallback quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = optimization_config\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "tflite_model_float_fallback = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyCemb1F3n_z"
      },
      "source": [
        "### Convert using integer-only quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H53110LWwOW4",
        "outputId": "79900289-1f18-4f14-b415-64ad820f05ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
            "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
            "WARNING:absl:Buffer deduplication procedure will be skipped when flatbuffer library is not properly loaded\n"
          ]
        }
      ],
      "source": [
        "#TF Lite model with Full integer quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = optimization_config\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to int8 \n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MizihKRI3n_z"
      },
      "source": [
        "### Save the models as files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffr_JCfE3n_z"
      },
      "source": [
        "You'll need a `.tflite` file to deploy your model on other devices. So let's save the converted models to files and then load them when we run inferences below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvJ__uWZwSFB",
        "outputId": "364c66b7-75bf-40db-ea4a-3e6aed2c9578"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "627696"
            ]
          },
          "execution_count": 40,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/content/rps_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the unquantized/float model:\n",
        "\n",
        "tflite_model_file = tflite_models_dir/\"rps_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "\n",
        "# Save the dynamic range quantized model:\n",
        "\n",
        "tf_model_dynamic_range_file = tflite_models_dir/\"rps_model_dynamic_range.tflite\"\n",
        "tf_model_dynamic_range_file.write_bytes(tflite_model_dynamic_range)\n",
        "\n",
        "# Save the float fallback quantized model:\n",
        "\n",
        "tflite_model_float_fallback_file = tflite_models_dir/\"rps_model_float_fallback.tflite\"\n",
        "tflite_model_float_fallback_file.write_bytes(tflite_model_float_fallback)\n",
        "\n",
        "# Save the integer only quantized model:\n",
        "\n",
        "tflite_model_quant_file = tflite_models_dir/\"rps_model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSeUQzre54Iv"
      },
      "source": [
        "### Test the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWn82C2xwYUJ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(tflite_file, dataset, model_type):\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "\n",
        "  interpreter.allocate_tensors()\n",
        "  \n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details =  interpreter.get_output_details()[0]\n",
        "\n",
        "  total_seen = 0\n",
        "  num_correct = 0\n",
        "  is_int8_quantized = (input_details['dtype'] == np.int8)\n",
        "  \n",
        "  for image_batch, labels_batch in dataset:\n",
        "    for i in range(tf.shape(image_batch)[0]):\n",
        "      test_image = image_batch[i]\n",
        "      \n",
        "\n",
        "      if is_int8_quantized:\n",
        "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "        test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "      test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "      interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "      interpreter.invoke()\n",
        "      output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "      output = np.argmax(output)\n",
        "\n",
        "      if labels_batch[i] == output:\n",
        "        num_correct += 1\n",
        "      total_seen += 1\n",
        "\n",
        "      if total_seen % 50 == 0:\n",
        "        print(\"Accuracy after %i images: %f\" %\n",
        "              (total_seen, float(num_correct) / float(total_seen)))\n",
        "  print('Num images: {0:}, Accuracy: {1:.4f}, Type: {2:}'.format(total_seen, float(num_correct / total_seen), model_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUrX2cX-wbIL",
        "outputId": "8837025d-7bc2-453a-e508-5cef8e293c37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy after 50 images: 0.800000\n",
            "Num images: 64, Accuracy: 0.7969, Type: Float\n",
            "Accuracy after 50 images: 0.920000\n",
            "Num images: 64, Accuracy: 0.9219, Type: Dynamic Range\n",
            "Accuracy after 50 images: 0.960000\n",
            "Num images: 64, Accuracy: 0.9688, Type: Float Fallback\n",
            "Accuracy after 50 images: 0.960000\n",
            "Num images: 64, Accuracy: 0.9531, Type: Integer Quantized\n",
            "2/2 [==============================] - 1s 68ms/step - loss: 0.2270 - accuracy: 0.9375\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.2269519865512848, 0.9375]"
            ]
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Check accuracy on the test subset\n",
        "evaluate_model(tflite_model_file, test_dataset, model_type=\"Float\")\n",
        "evaluate_model(tf_model_dynamic_range_file, test_dataset, model_type=\"Dynamic Range\")\n",
        "evaluate_model(tflite_model_float_fallback_file, test_dataset, model_type=\"Float Fallback\")\n",
        "evaluate_model(tflite_model_quant_file, test_dataset, model_type=\"Integer Quantized\")\n",
        "model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pH7j9yeweIW",
        "outputId": "4496c751-ba0a-45fc-9103-3b614cbc40e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy after 50 images: 0.920000\n",
            "Accuracy after 100 images: 0.930000\n",
            "Accuracy after 150 images: 0.920000\n",
            "Accuracy after 200 images: 0.905000\n",
            "Accuracy after 250 images: 0.924000\n",
            "Accuracy after 300 images: 0.920000\n",
            "Num images: 308, Accuracy: 0.9188, Type: Float\n",
            "Accuracy after 50 images: 0.980000\n",
            "Accuracy after 100 images: 0.990000\n",
            "Accuracy after 150 images: 0.966667\n",
            "Accuracy after 200 images: 0.940000\n",
            "Accuracy after 250 images: 0.948000\n",
            "Accuracy after 300 images: 0.933333\n",
            "Num images: 308, Accuracy: 0.9351, Type: Dynamic Range\n",
            "Accuracy after 50 images: 0.980000\n",
            "Accuracy after 100 images: 0.980000\n",
            "Accuracy after 150 images: 0.980000\n",
            "Accuracy after 200 images: 0.980000\n",
            "Accuracy after 250 images: 0.972000\n",
            "Accuracy after 300 images: 0.970000\n",
            "Num images: 308, Accuracy: 0.9708, Type: Float Fallback\n",
            "Accuracy after 50 images: 0.980000\n",
            "Accuracy after 100 images: 0.980000\n",
            "Accuracy after 150 images: 0.986667\n",
            "Accuracy after 200 images: 0.965000\n",
            "Accuracy after 250 images: 0.968000\n",
            "Accuracy after 300 images: 0.970000\n",
            "Num images: 308, Accuracy: 0.9675, Type: Integer Quantized\n",
            "10/10 [==============================] - 1s 15ms/step - loss: 0.2926 - accuracy: 0.9188\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[0.29259738326072693, 0.9188311696052551]"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#Check accuracy on all validation data\n",
        "evaluate_model(tflite_model_file, validation_dataset, model_type=\"Float\")\n",
        "evaluate_model(tf_model_dynamic_range_file, validation_dataset, model_type=\"Dynamic Range\")\n",
        "evaluate_model(tflite_model_float_fallback_file, validation_dataset, model_type=\"Float Fallback\")\n",
        "evaluate_model(tflite_model_quant_file, validation_dataset, model_type=\"Integer Quantized\")\n",
        "model.evaluate(validation_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sc2VDnhwhxj",
        "outputId": "b25d0b40-5295-4a39-c4a7-8e9854681c63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Float model in KB: 1573.6015625\n",
            "Dynamic Range model in KB: 592.25\n",
            "Float fallback model in KB: 613.3046875\n",
            "Integer Quantized model in KB: 612.984375\n"
          ]
        }
      ],
      "source": [
        "print(\"Float model in KB:\", os.path.getsize(tflite_model_file) / float(2**10))\n",
        "print(\"Dynamic Range model in KB:\", os.path.getsize(tf_model_dynamic_range_file) / float(2**10))\n",
        "print(\"Float fallback model in KB:\", os.path.getsize(tflite_model_float_fallback_file) / float(2**10))\n",
        "print(\"Integer Quantized model in KB:\", os.path.getsize(tflite_model_quant_file) / float(2**10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJwR0DKO60sU"
      },
      "source": [
        "So you now have an integer quantized a model with almost no difference in the accuracy, compared to the float model.\n",
        "\n",
        "To learn more about other quantization strategies, read about [TensorFlow Lite model optimization](https://www.tensorflow.org/lite/performance/model_optimization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3B_DNmm3n_0"
      },
      "source": [
        "## Generate a TensorFlow Lite for MicroControllers Model\n",
        "Convert the TensorFlow Lite model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPS179IlwUIY",
        "outputId": "7fe3a04e-4529-4e33-8967-d982a47a65c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease\n",
            "Get:2 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:3 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease\n",
            "Hit:7 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:11 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease\n",
            "Hit:12 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:14 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [2,095 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [1,073 kB]\n",
            "Fetched 3,272 kB in 5s (694 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
        "MODEL_TFLITE = \"/content/rps_tflite_models/rps_model_quant.tflite\"\n",
        "MODEL_TFLITE_MICRO = \"/content/rps_tflite_models/rps_model_quant.cc\"\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/rps_model_tflite/g' {MODEL_TFLITE_MICRO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA2Md_eF3n_0"
      },
      "source": [
        "### Deploy to a Microcontroller\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmUjTfpzCHCJ"
      },
      "source": [
        "First we download the genrated model for MCU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NYd9vJ9-zHqm",
        "outputId": "0f33aa57-480c-482f-90ef-08f80ff7ae5c"
      },
      "outputs": [
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f7a4c853-5614-4e76-af3a-2eff00ec9fed\", \"rps_model_quant.cc\", 3870875)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/rps_tflite_models/rps_model_quant.cc') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR56P8mlCMir"
      },
      "source": [
        "We now need to move to PC side.\n",
        "\n",
        "1. Clone the code repository from GitHub to your PC. For example, if you use the shell command:\n",
        "```\n",
        "git clone https://github.com/SuperChange001/deeplearning_labs.git\n",
        "```\n",
        "2. Open the folder `Lab03/esp32-projects/rock_paper_scissors` in vscode.\n",
        "3. Building the project and flash to the MCU, assuming you have installed the extension, you will see the buttons at the left-bottom of the vs code editor:\n",
        "\n",
        "![esp-idf](https://i.imgur.com/jkzDUXA.png)\n",
        "\n",
        "- *1*: choose the correct usb to serial device\n",
        "- *2*: clean the build\n",
        "- *3*: build the project\n",
        "- *4*: flash the build binary file to the ESP32\n",
        "- *5*: Monitor the log information from the ESP32\n",
        "- *6*: build+flash+monitor\n",
        "\n",
        "*Tips:* To connect the hardware correctly, you need to specify the `USB device` (`/dev/ttyUSBxxx` on Linux, or `COM port` device on windows), e.g.\n",
        "\n",
        "![interface select](https://s3.bmp.ovh/imgs/2022/08/24/9842faefa5a9cd70.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f29Wnd8Ukb8M"
      },
      "source": [
        "## What is the next?\n",
        "1. Try your own model on the MCU!\n",
        "  - This means replacing the `rps_quant.cc` in `panth_to_your_clone/Lab03/esp32-projects/rock_paper_scissors/main` with the file generated by you.\n",
        "  - Change the tensor arena size in file `main_functions.cc`:\n",
        "    ```\n",
        "    constexpr int kTensorArenaSize = 530 * 1024;\n",
        "    ```\n",
        "    which defines the size of the memory area in which the calculations of the model happen according to the needs of your model. Thankfully, if its too small, the esp will tell you during runtime that this is the case, and how roughly how much is missing, so you can get to a somewhat optimal tensor arena size by trial-and-error.\n",
        "  - Add necessary operations in your `main_functions.cc` depending on your model structure:  \n",
        "  \n",
        "    ```\n",
        "    // Pull in only the operation implementations we need.\n",
        "    // This relies on a complete list of all the ops needed by this graph.\n",
        "    // An easier approach is to just use the AllOpsResolver, but this will\n",
        "    // incur some penalty in code space for op implementations that are not\n",
        "    // needed by this graph.\n",
        "    //\n",
        "    // tflite::AllOpsResolver resolver;\n",
        "    // NOLINTNEXTLINE(runtime-global-variables)\n",
        "    static tflite::MicroMutableOpResolver<5> micro_op_resolver;\n",
        "    micro_op_resolver.AddAveragePool2D();\n",
        "    micro_op_resolver.AddConv2D();\n",
        "    micro_op_resolver.AddDepthwiseConv2D();\n",
        "    micro_op_resolver.AddReshape();\n",
        "    micro_op_resolver.AddSoftmax();\n",
        "    ```\n",
        "    *A list of supported operations can be found [here](https://github.com/SuperChange001/deeplearning_labs/blob/main/Lab03/esp32-projects/rock_paper_scissors/components/tfmicro/tensorflow/lite/micro/all_ops_resolver.cc)*.\n",
        "\n",
        "2. For a different image recognition task, you may also need to change the `kCategoryLabels` field in the file `model_settings.cc`. *In the `model_setting.h`, more options you can tweak, do you know what all these parameters mean?*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Lab03.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}