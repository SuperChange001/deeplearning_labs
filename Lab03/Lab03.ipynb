{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rcFhSB8lde8B"
      },
      "source": [
        "# Lab 03: Embedded Deep Learning\n",
        "\n",
        "In this lab session, we will optimize the deep learning model that was trained in the last session. Later we will put the quantized model to the ESP32 MCU.\n",
        "\n",
        "![ESP-EYE](https://media.digikey.com/Photos/Espressif%20Systems/MFG_ESP-EYE_Top.jpg)\n",
        "\n",
        "The first half session can be run on google colab. Open in google colab -> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg#left)](https://colab.research.google.com/github/SuperChange001/deeplearning_labs/blob/main/Lab03/Lab03.ipynb)\n",
        "\n",
        "But the latter part must be running on your PC, below are tutorials of installing required softwares for your PC:\n",
        "1. [VS Code Editor](https://code.visualstudio.com/download)\n",
        "2. [Git Client](https://git-scm.com/downloads/guis)\n",
        "3. [ESP-IDF](https://github.com/espressif/vscode-esp-idf-extension/blob/master/docs/tutorial/install.md)\n",
        "\n",
        "*Tips:* It is recommended to try to install all the software before you come to the Lab. But, don't worry if you fail to install everything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2EV4SBCecxWZ"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP98q8pBb0vc"
      },
      "source": [
        "First, we import some libraries for image processing and utils, as well as TensorFlow. Note that the module `image_dataset_from_directory` is necessary for downloading our data set from Google."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpyxHufwFKYp"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
        "\n",
        "# Set the seed value for experiment reproducibility.\n",
        "seed = 32\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPfEkbUYG_ci"
      },
      "source": [
        "## Import the Gesture dataset for evaluating the pretrianed model\n",
        "\n",
        "Download and extract the `zip` file containing the datasets with `tf.keras.utils.get_file`. \n",
        "\n",
        "*Tips: change the code respectively if you have a model for other task.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oy8hBBOCMlA0"
      },
      "outputs": [],
      "source": [
        "# Download our dataset used for training\n",
        "TRAIN_SET_URL = 'https://storage.googleapis.com/learning-datasets/rps.zip'\n",
        "path_to_zip = tf.keras.utils.get_file('rps.zip', origin=TRAIN_SET_URL, extract=True, cache_dir='/content')\n",
        "train_dir = os.path.join(os.path.dirname(path_to_zip), \"rps\")\n",
        "\n",
        "# As well as the validation dataset\n",
        "VAL_SET_URL = 'https://storage.googleapis.com/learning-datasets/rps-test-set.zip'\n",
        "path_to_zip2 = tf.keras.utils.get_file('rps-test-set.zip', origin=VAL_SET_URL, extract=True, cache_dir='/content')\n",
        "validation_dir = os.path.join(os.path.dirname(path_to_zip2), \"rps-test-set\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0HBernrNHwFc"
      },
      "source": [
        "Then we can generate tf.data.Dataset from image files in a directory.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E0HnHBGmNLh5",
        "outputId": "355a75dd-9b80-4bd2-ddae-5701f281ded2"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 32\n",
        "IMG_SIZE = (96, 96) \n",
        "\n",
        "train_dataset = image_dataset_from_directory(train_dir,\n",
        "                                             shuffle=True,\n",
        "                                             batch_size=BATCH_SIZE,\n",
        "                                             image_size=IMG_SIZE)\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(validation_dir,\n",
        "                                                  shuffle=True,\n",
        "                                                  batch_size=BATCH_SIZE,\n",
        "                                                  image_size=IMG_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRHrIWLUJDQR"
      },
      "source": [
        "Lets display some images of our dataset, as well as the class names."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGJ5uDKeJuvN"
      },
      "source": [
        "### Split test set and validation set\n",
        "We are now taking a fifth of the validation dataset to use as our test set. The validation set will be used for observing if we got overfitting during training while the test set is for the final test after training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kkbBixY8NVbP",
        "outputId": "02077a02-78d4-497a-9970-b6bf233e1276"
      },
      "outputs": [],
      "source": [
        "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
        "\n",
        "test_dataset = validation_dataset.take(val_batches // 5)\n",
        "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
        "\n",
        "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
        "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VSjEqLOIspiZ"
      },
      "source": [
        "## Upload model and load the model from hard disk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXUFpmD03n_v"
      },
      "source": [
        "Fetch the example model from github, you can also upload your own model to colab:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kY9rzyf5t1gj"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "import h5py\n",
        "import requests\n",
        "\n",
        "url = 'https://github.com/SuperChange001/deeplearning_labs/raw/main/Lab03/pretrained_models/model_rps.h5'\n",
        "r = requests.get(url, allow_redirects=True)\n",
        "\n",
        "with open('/content/model_rps.h5', 'wb') as f:\n",
        "    f.write(r.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VAA6tMyZ3n_w"
      },
      "source": [
        "Alternative way for fetching the pretrained model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gP_iYinhrShE",
        "outputId": "693486ec-fd83-465e-ee92-94fd68b6bdab"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/SuperChange001/deeplearning_labs/raw/main/Lab03/pretrained_models/model_rps.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22HmS5zM3n_w"
      },
      "source": [
        "Load model from hard disk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wzU3N-MsqeQ",
        "outputId": "350e7ed7-38e2-4330-caef-4a7e5f47980a"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model('/content/model_rps.h5')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6yeFoHyg3n_x"
      },
      "source": [
        "Test Accuracy of the loaded model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1OfCsJHwAx0",
        "outputId": "0419e4e9-e856-4d4a-a2a2-1b3e376050ab"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_dataset)\n",
        "print('Test accuracy :', accuracy)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFWEgNO83n_x"
      },
      "source": [
        "## Convert model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SI6M0VxC3n_y"
      },
      "source": [
        "### Convert to a TensorFlow Lite model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nJA1ep4S3n_y",
        "outputId": "28de8cfb-5a4c-4209-c8d5-28ae6a1c6fb8"
      },
      "outputs": [],
      "source": [
        "#TF Lite model without quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "tflite_model = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OJtFP0rd3n_y"
      },
      "source": [
        "### Convert using dynamic range quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QlYVUayf3n_y",
        "outputId": "0c08b8e2-cdf6-4e06-d2ef-b0a558aa33ec"
      },
      "outputs": [],
      "source": [
        "# Parameters setting\n",
        "optimization_config = [tf.lite.Optimize.DEFAULT]\n",
        "\n",
        "#TF Lite model with dynamic range quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = optimization_config\n",
        "\n",
        "tflite_model_dynamic_range = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Byb0_jnX3n_y"
      },
      "source": [
        "### Convert using float fallback quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHI_tjr93n_y",
        "outputId": "2052439c-5ed3-4bcf-c467-2e35c0186aec"
      },
      "outputs": [],
      "source": [
        "#Extracts sample images needed for float fallback and full integer quantization \n",
        "def representative_data_gen():\n",
        "  for input in train_dataset.take(4):\n",
        "    for input_value in tf.data.Dataset.from_tensor_slices(np.array(input[0])).batch(1).take(32):\n",
        "      yield [input_value]\n",
        "\n",
        "#TF Lite model with Float Fallback quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = optimization_config\n",
        "converter.representative_dataset = representative_data_gen\n",
        "\n",
        "tflite_model_float_fallback = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyCemb1F3n_z"
      },
      "source": [
        "### Convert using integer-only quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H53110LWwOW4",
        "outputId": "79900289-1f18-4f14-b415-64ad820f05ff"
      },
      "outputs": [],
      "source": [
        "#TF Lite model with Full integer quantization\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = optimization_config\n",
        "converter.representative_dataset = representative_data_gen\n",
        "# Ensure that if any ops can't be quantized, the converter throws an error\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "# Set the input and output tensors to int8 \n",
        "converter.inference_input_type = tf.int8\n",
        "converter.inference_output_type = tf.int8\n",
        "\n",
        "tflite_model_quant = converter.convert()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MizihKRI3n_z"
      },
      "source": [
        "### Save the models as files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ffr_JCfE3n_z"
      },
      "source": [
        "You'll need a `.tflite` file to deploy your model on other devices. So let's save the converted models to files and then load them when we run inferences below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gvJ__uWZwSFB",
        "outputId": "364c66b7-75bf-40db-ea4a-3e6aed2c9578"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "\n",
        "tflite_models_dir = pathlib.Path(\"/content/rps_tflite_models/\")\n",
        "tflite_models_dir.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "# Save the unquantized/float model:\n",
        "\n",
        "tflite_model_file = tflite_models_dir/\"rps_model.tflite\"\n",
        "tflite_model_file.write_bytes(tflite_model)\n",
        "\n",
        "# Save the dynamic range quantized model:\n",
        "\n",
        "tf_model_dynamic_range_file = tflite_models_dir/\"rps_model_dynamic_range.tflite\"\n",
        "tf_model_dynamic_range_file.write_bytes(tflite_model_dynamic_range)\n",
        "\n",
        "# Save the float fallback quantized model:\n",
        "\n",
        "tflite_model_float_fallback_file = tflite_models_dir/\"rps_model_float_fallback.tflite\"\n",
        "tflite_model_float_fallback_file.write_bytes(tflite_model_float_fallback)\n",
        "\n",
        "# Save the integer only quantized model:\n",
        "\n",
        "tflite_model_quant_file = tflite_models_dir/\"rps_model_quant.tflite\"\n",
        "tflite_model_quant_file.write_bytes(tflite_model_quant)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSeUQzre54Iv"
      },
      "source": [
        "### Test the models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JWn82C2xwYUJ"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(tflite_file, dataset, model_type):\n",
        "  interpreter = tf.lite.Interpreter(model_path=str(tflite_file))\n",
        "\n",
        "  interpreter.allocate_tensors()\n",
        "  \n",
        "  input_details = interpreter.get_input_details()[0]\n",
        "  output_details =  interpreter.get_output_details()[0]\n",
        "\n",
        "  total_seen = 0\n",
        "  num_correct = 0\n",
        "  is_int8_quantized = (input_details['dtype'] == np.int8)\n",
        "  \n",
        "  for image_batch, labels_batch in dataset:\n",
        "    for i in range(tf.shape(image_batch)[0]):\n",
        "      test_image = image_batch[i]\n",
        "      \n",
        "\n",
        "      if is_int8_quantized:\n",
        "        input_scale, input_zero_point = input_details[\"quantization\"]\n",
        "        test_image = test_image / input_scale + input_zero_point\n",
        "\n",
        "      test_image = np.expand_dims(test_image, axis=0).astype(input_details[\"dtype\"])\n",
        "      interpreter.set_tensor(input_details[\"index\"], test_image)\n",
        "      interpreter.invoke()\n",
        "      output = interpreter.get_tensor(output_details[\"index\"])[0]\n",
        "\n",
        "      output = np.argmax(output)\n",
        "\n",
        "      if labels_batch[i] == output:\n",
        "        num_correct += 1\n",
        "      total_seen += 1\n",
        "\n",
        "      if total_seen % 50 == 0:\n",
        "        print(\"Accuracy after %i images: %f\" %\n",
        "              (total_seen, float(num_correct) / float(total_seen)))\n",
        "  print('Num images: {0:}, Accuracy: {1:.4f}, Type: {2:}'.format(total_seen, float(num_correct / total_seen), model_type))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUrX2cX-wbIL",
        "outputId": "8837025d-7bc2-453a-e508-5cef8e293c37"
      },
      "outputs": [],
      "source": [
        "#Check accuracy on the test subset\n",
        "evaluate_model(tflite_model_file, test_dataset, model_type=\"Float\")\n",
        "evaluate_model(tf_model_dynamic_range_file, test_dataset, model_type=\"Dynamic Range\")\n",
        "evaluate_model(tflite_model_float_fallback_file, test_dataset, model_type=\"Float Fallback\")\n",
        "evaluate_model(tflite_model_quant_file, test_dataset, model_type=\"Integer Quantized\")\n",
        "model.evaluate(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pH7j9yeweIW",
        "outputId": "4496c751-ba0a-45fc-9103-3b614cbc40e1"
      },
      "outputs": [],
      "source": [
        "#Check accuracy on all validation data\n",
        "evaluate_model(tflite_model_file, validation_dataset, model_type=\"Float\")\n",
        "evaluate_model(tf_model_dynamic_range_file, validation_dataset, model_type=\"Dynamic Range\")\n",
        "evaluate_model(tflite_model_float_fallback_file, validation_dataset, model_type=\"Float Fallback\")\n",
        "evaluate_model(tflite_model_quant_file, validation_dataset, model_type=\"Integer Quantized\")\n",
        "model.evaluate(validation_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspecting the models' size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2sc2VDnhwhxj",
        "outputId": "b25d0b40-5295-4a39-c4a7-8e9854681c63"
      },
      "outputs": [],
      "source": [
        "print(\"Float model in KB:\", os.path.getsize(tflite_model_file) / float(2**10))\n",
        "print(\"Dynamic Range model in KB:\", os.path.getsize(tf_model_dynamic_range_file) / float(2**10))\n",
        "print(\"Float fallback model in KB:\", os.path.getsize(tflite_model_float_fallback_file) / float(2**10))\n",
        "print(\"Integer Quantized model in KB:\", os.path.getsize(tflite_model_quant_file) / float(2**10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJwR0DKO60sU"
      },
      "source": [
        "So you now have an integer quantized a model with almost no difference in the accuracy, compared to the float model.\n",
        "\n",
        "To learn more about other quantization strategies, read about [TensorFlow Lite model optimization](https://www.tensorflow.org/lite/performance/model_optimization)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3B_DNmm3n_0"
      },
      "source": [
        "## Generate a TensorFlow Lite for MicroControllers Model\n",
        "Convert the TensorFlow Lite model into a C source file that can be loaded by TensorFlow Lite for Microcontrollers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPS179IlwUIY",
        "outputId": "7fe3a04e-4529-4e33-8967-d982a47a65c4"
      },
      "outputs": [],
      "source": [
        "# Install xxd if it is not available\n",
        "!apt-get update && apt-get -qq install xxd\n",
        "# Convert to a C source file, i.e, a TensorFlow Lite for Microcontrollers model\n",
        "MODEL_TFLITE = \"/content/rps_tflite_models/rps_model_quant.tflite\"\n",
        "MODEL_TFLITE_MICRO = \"/content/rps_tflite_models/rps_model_quant.cc\"\n",
        "!xxd -i {MODEL_TFLITE} > {MODEL_TFLITE_MICRO}\n",
        "# Update variable names\n",
        "REPLACE_TEXT = MODEL_TFLITE.replace('/', '_').replace('.', '_')\n",
        "!sed -i 's/'{REPLACE_TEXT}'/rps_model_tflite/g' {MODEL_TFLITE_MICRO}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IA2Md_eF3n_0"
      },
      "source": [
        "### Deploy to a Microcontroller\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RmUjTfpzCHCJ"
      },
      "source": [
        "First we download the genrated model for MCU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "NYd9vJ9-zHqm",
        "outputId": "0f33aa57-480c-482f-90ef-08f80ff7ae5c"
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download('/content/rps_tflite_models/rps_model_quant.cc') "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NR56P8mlCMir"
      },
      "source": [
        "We now need to move to PC side.\n",
        "\n",
        "1. Clone the code repository from GitHub to your PC. For example, if you use the shell command:\n",
        "```\n",
        "git clone https://github.com/SuperChange001/deeplearning_labs.git\n",
        "```\n",
        "2. Open the folder `Lab03/esp32-projects/rock_paper_scissors` in vscode.\n",
        "3. Building the project and flash to the MCU, assuming you have installed the extension, you will see the buttons at the left-bottom of the vs code editor:\n",
        "\n",
        "![esp-idf](https://i.imgur.com/jkzDUXA.png)\n",
        "\n",
        "- *1*: choose the correct usb to serial device\n",
        "- *2*: clean the build\n",
        "- *3*: build the project\n",
        "- *4*: flash the build binary file to the ESP32\n",
        "- *5*: Monitor the log information from the ESP32\n",
        "- *6*: build+flash+monitor\n",
        "\n",
        "*Tips:* To connect the hardware correctly, you need to specify the `USB device` (`/dev/ttyUSBxxx` on Linux, or `COM port` device on windows), e.g.\n",
        "\n",
        "![interface select](https://s3.bmp.ovh/imgs/2022/08/24/9842faefa5a9cd70.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f29Wnd8Ukb8M"
      },
      "source": [
        "## What is the next?\n",
        "1. Try your own model on the MCU!\n",
        "  - This means replacing the `rps_quant.cc` in `panth_to_your_clone/Lab03/esp32-projects/rock_paper_scissors/main` with the file generated by you.\n",
        "  - Change the tensor arena size in file `main_functions.cc`:\n",
        "    ```\n",
        "    constexpr int kTensorArenaSize = 530 * 1024;\n",
        "    ```\n",
        "    which defines the size of the memory area in which the calculations of the model happen according to the needs of your model. Thankfully, if its too small, the esp will tell you during runtime that this is the case, and how roughly how much is missing, so you can get to a somewhat optimal tensor arena size by trial-and-error.\n",
        "  - Add necessary operations in your `main_functions.cc` depending on your model structure:  \n",
        "  \n",
        "    ```\n",
        "    // Pull in only the operation implementations we need.\n",
        "    // This relies on a complete list of all the ops needed by this graph.\n",
        "    // An easier approach is to just use the AllOpsResolver, but this will\n",
        "    // incur some penalty in code space for op implementations that are not\n",
        "    // needed by this graph.\n",
        "    //\n",
        "    // tflite::AllOpsResolver resolver;\n",
        "    // NOLINTNEXTLINE(runtime-global-variables)\n",
        "    static tflite::MicroMutableOpResolver<5> micro_op_resolver;\n",
        "    micro_op_resolver.AddAveragePool2D();\n",
        "    micro_op_resolver.AddConv2D();\n",
        "    micro_op_resolver.AddDepthwiseConv2D();\n",
        "    micro_op_resolver.AddReshape();\n",
        "    micro_op_resolver.AddSoftmax();\n",
        "    ```\n",
        "    *A list of supported operations can be found [here](https://github.com/SuperChange001/deeplearning_labs/blob/main/Lab03/esp32-projects/rock_paper_scissors/components/tfmicro/tensorflow/lite/micro/all_ops_resolver.cc)*.\n",
        "\n",
        "2. For a different image recognition task, you may also need to change the `kCategoryLabels` field in the file `model_settings.cc`. *In the `model_setting.h`, more options you can tweak, do you know what all these parameters mean?*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Lab03.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.10.5 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.5"
    },
    "vscode": {
      "interpreter": {
        "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
