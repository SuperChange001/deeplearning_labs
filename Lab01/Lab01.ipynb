{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YI5ZaJyNd19K",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 01: First Steps with AI Programming\n",
    "\n",
    "In this lab session, we will guide you to start AI programming.\n",
    "This lab session includes:\n",
    "- Google Colab introduction \n",
    "- Programming with TensorFlow Keras\n",
    "  - Creating models\n",
    "  - Creating custom dataset\n",
    "  - Data augmentation\n",
    "  - Train/evaluate/test model\n",
    "  - Result analysis and visulization\n",
    "\n",
    "Open in google colab -> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg#left)](https://colab.research.google.com/github/SuperChange001/deeplearning_labs/blob/main/Lab01/Lab01.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4o5J3f1IrsVl",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Set up TensorFlow\n",
    "Import TensorFlow into your program to get started: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UL372hPQd19O",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 12\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UpnrddrDsNUA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Here we check if GPU is available. For this Lab session, it is not very critical, but for future Labs enabling GPU will make our life easier.\n",
    "\n",
    "\n",
    "- To enable the GPU **Runtime** -> **Change runtime type** -> **Hardware accelerator** -> **GPU**.\n",
    "- After you excute the code block below, you should see similiar output like: `PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BMm-gIZnpAA0",
    "outputId": "4128f912-cc9f-4fd9-fcb3-e4a8338b4bc6",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-aNxYoNQuKjX",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Create our First AI model\n",
    "Let's first have a look at a very simple AI model, an [multilayer perceptron](https://www.sciencedirect.com/topics/computer-science/multilayer-perceptron):\n",
    "![image](https://i.imgur.com/NQLAt6y.png)\n",
    "\n",
    "three inputs `I1`, `I2`, and `I3` are directly fed into the so-called input layer while the column of blue nodes represents the hidden layer which contains 3 neurons. Then the last output layer with 2 neurons.\n",
    "\n",
    "To create such an AI model with Keras we only need 7 lines of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9nM7C51gelNC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        layers.Input(3, name=\"input_layer\"),\n",
    "        layers.Dense(2, activation=\"relu\", name=\"hidden_layer_1\"),\n",
    "        layers.Dense(1, activation=\"sigmoid\", name=\"ouput_layer\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HMb6hSJA5_W0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "To check if the model is as expected, we can print a string summary of the network(model):\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b8Rdxig15-kY",
    "outputId": "480f4fe2-669f-4653-be32-60da12521d34",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlLZA2yoEeUh",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "A quick example of how to feed data to the model and get the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p50cyyKkD8mw",
    "outputId": "9d7aff6d-e89a-43e7-94f7-28877cd9bb79",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs = [[1,2,3], [2,3,4]]\n",
    "\n",
    "outputs = model.predict(inputs)\n",
    "\n",
    "print(\"inputs\\n\", inputs)\n",
    "print(\"\\noutputs\\n\",outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QhxK8at0GEgk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Even if you change the numbers above, the pattern between output its input will still look unrelated. Nevertheless, the behaviour of the untrained AI model is designed like this since its parameters are initialised with random numbers.\n",
    "\n",
    "To train the model we need a data set. In the following section, we will first formulate a logic function and generate the dataset for training and testing our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HGeM1vRp6tb7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Logic Functions and its Truth Table\n",
    "Therefore, we define a logic function as below: \n",
    "\n",
    "\\begin{equation}\n",
    "O=(\\overline{I1} \\cdot I2 \\cdot I3)+(I1 \\cdot \\overline{I2} \\cdot I3)+(I1 \\cdot I2 \\cdot \\overline{I3})\n",
    "\\end{equation}\n",
    "\n",
    "Here, `I1`, `I2` and `I3` are three independent inputs, and `O` is the only output. By feeding in all combinations of inputs we can build up the truth table of our logic function:\n",
    "\n",
    "\n",
    "| I1| I2| I3| O |\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 1 | 0 |\n",
    "| 0 | 1 | 0 | 0 |\n",
    "| 0 | 1 | 1 | 1 |\n",
    "| 1 | 0 | 0 | 0 |\n",
    "| 1 | 0 | 1 | 1 |\n",
    "| 1 | 1 | 0 | 1 |\n",
    "| 1 | 1 | 1 | 0 |\n",
    "\n",
    "Now we can create the truth tables in Python, the prefix `x_` stands for the inputs and `y_` stands for expected output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy2froS4Lk4C",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np # the fundamental package for array computing with Python.\n",
    "\n",
    "x_truth_table = np.array([[0,0,0], \n",
    "           [0,0,1], \n",
    "           [0,1,0], \n",
    "           [0,1,1], \n",
    "           [1,0,0], \n",
    "           [1,0,1], \n",
    "           [1,1,0], \n",
    "           [1,1,1]])\n",
    "y_truth_table = np.array([[0],\n",
    "           [0],\n",
    "           [0],\n",
    "           [1],\n",
    "           [0],\n",
    "           [1],\n",
    "           [1],\n",
    "           [0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSbkg1ndMw4H",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Data Augmentation\n",
    "Though with only 8 entries of data is possible to train the AI model, it often fails. Therefore we prefer to generate more data from our defined truth table. One easy way to generate more data is by adding some noise to the existing data set.\n",
    "\n",
    "1. we duplicate the data by [numpy.tile](https://numpy.org/doc/stable/reference/generated/numpy.tile.html), to have enough enetries. \n",
    "\n",
    "*Tips: from the reference you will also find the meaning of `duplicate_factor`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oTPnLl3JBuYT",
    "outputId": "77fefec9-e67c-4f3e-e1ef-be75775ca93e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "duplicate_factor_train = 60\n",
    "noise_amplitude_factor = 0.2\n",
    "\n",
    "x_duplicated = np.tile(x_truth_table,(duplicate_factor_train, 1))\n",
    "y_duplicated = np.tile(y_truth_table,(duplicate_factor_train, 1))\n",
    "\n",
    "print(\"The size of the data set in increased from %d to %d.\" %  (len(x_truth_table), len(x_duplicated)))\n",
    "\n",
    "train_x = (x_duplicated.copy()+noise_amplitude_factor*np.random.random_sample(x_duplicated.shape)) # add noise\n",
    "train_y = (y_duplicated.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rrRiZ5NHXmvL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Among the training set `train_x`, `train_y`, we also need another two data sets for validation and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cE6jmazgL6n",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Building the validation set\n",
    "duplicate_factor_val = 40\n",
    "noise_amplitude_factor_val = 0.2\n",
    "\n",
    "x_duplicated = np.tile(x_truth_table,(duplicate_factor_val, 1))\n",
    "y_duplicated = np.tile(y_truth_table,(duplicate_factor_val, 1))\n",
    "\n",
    "val_x = (x_duplicated.copy()+noise_amplitude_factor_val*np.random.random_sample(x_duplicated.shape))\n",
    "val_y = (y_duplicated.copy())\n",
    "\n",
    "# Building the test set\n",
    "duplicate_factor_test = 20\n",
    "noise_amplitude_factor_test = 0.2\n",
    "\n",
    "x_duplicated = np.tile(x_truth_table,(duplicate_factor_test, 1))\n",
    "y_duplicated = np.tile(y_truth_table,(duplicate_factor_test, 1))\n",
    "\n",
    "test_x = (x_duplicated.copy()+noise_amplitude_factor_test*np.random.random_sample(x_duplicated.shape))\n",
    "test_y = (y_duplicated.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CJWLmIareUL_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Training\n",
    "`epoches`: train the model by feeding the training set how many times?\n",
    "\n",
    "`batch_size`: number of samples(rows in the `train_x`) that will be propagated through the network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zoMjUiYZhRab",
    "outputId": "a647fd45-0539-411a-825a-8fb3adfbe0e0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "epoches = 10\n",
    "batch_size = 64\n",
    "\n",
    "loss_function = 'binary_crossentropy'\n",
    "optimizer = 'adam'\n",
    "metrics = ['accuracy']\n",
    "\n",
    "model.compile(loss=loss_function, optimizer=optimizer, metrics=metrics)\n",
    "\n",
    "train_history = model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    batch_size=batch_size,\n",
    "    epochs=epoches,\n",
    "    shuffle=True,\n",
    "    # We pass some validation for\n",
    "    # monitoring validation loss and metrics\n",
    "    # at the end of each epoch\n",
    "    validation_data=(val_x, val_y),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w78yziaphiqL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see from the log information that, the loss is decreasing with the accuracy arising during the training process. The records of loss and accuracy are also returned by the `fit()` method.\n",
    "\n",
    "The code below fetches data from train_history and visualizes the accuracy and loss curves during training.\n",
    "\n",
    "*Tips: [Interpreting Loss Curves](https://developers.google.com/machine-learning/testing-debugging/metrics/interpretic)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "7bYAEw0zI_84",
    "outputId": "097315c6-75d4-4c05-b4fc-cc27b9f7ba3a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define a function so we can reuse it later\n",
    "def draw_learning_curves(history):\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.plot(acc, label='Training Accuracy')\n",
    "  plt.plot(val_acc, label='Validation Accuracy')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.plot(loss, label='Training Loss')\n",
    "  plt.plot(val_loss, label='Validation Loss')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.ylabel('Cross Entropy')\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.show()\n",
    "\n",
    "draw_learning_curves(train_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6UcMmSy1j8x-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5vUcse1vm24l",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "`model.predict` is used to make the model predict output of the logic function.\n",
    "\n",
    "The return value of this methods provides all the predictions on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hvAFYolgkJ7t",
    "outputId": "d778e7ca-7d90-4484-d8e5-e6aaaf8bd215",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(test_x)\n",
    "print(y_pred[:5]) # print the first 5 predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f7Y5PEhnnVuU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The predictions are all floating point number, but we want the model to predict either the output will be '1' or '0'. One way to solve it, is appling a threshold and mapping the floating number to binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VcTtnOjkmtsg",
    "outputId": "463bafe4-09dd-4d97-eb68-ffd18731678c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred[y_pred < 0.5] = 0\n",
    "y_pred[y_pred >= 0.5] = 1\n",
    "\n",
    "print(y_pred[:5]) # print first 5, for a quick check.\n",
    "print(test_y[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeS4bYe9n8_7",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Another very popular way to visualize the test result is introducing the [confusion matrix](https://www.sciencedirect.com/topics/engineering/confusion-matrix#:~:text=A%20confusion%20matrix%20is%20a,malignant%20tissue%20is%20considered%20cancerous). With confusion matrix, we can easily tell which class is misclassfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "id": "Cjb2ozN-FVyM",
    "outputId": "5507cf03-c3f0-40da-cd86-0e026465d9c4",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "labels = [\"0\", \"1\"]\n",
    "\n",
    "cm = confusion_matrix(test_y, y_pred)\n",
    "# cm = confusion_matrix(test_y, y_pred, normalize=\"pred\") # What the normalized confusion matrix will look like?\n",
    "\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWO2Sp82p1Jy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Improvement\n",
    "So far the trained model performs still not good, here are several aspects we can optimize to improve the model:\n",
    "- [ ] larger dataset\n",
    "- [ ] different model structure\n",
    "  - [ ] more neurons\n",
    "  - [ ] more layers\n",
    "  - [ ] other activation functions\n",
    "- [ ] training process\n",
    "  - [ ] with more epochs, larger batch_size\n",
    "  - [ ] other loss function\n",
    "  - [ ] other evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pQ-kt-GdsSN0",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create a larger data set\n",
    "\n",
    "**Hint**: \n",
    "1. We need newly defined train_x, train_y, val_x, test_x, test_y\n",
    "2. You can copy the code from ealy part\n",
    "3. Try different `the duplicate_factor`, `noise_amplitude_factor` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YB-TUn5_p00S",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start Larger data set building\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# End of Larger data set building"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHsWoqJ0xF_y",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Create the second model\n",
    "\n",
    "**Hint**: \n",
    "1. Copy the code from the early part or you can try [other ways](https://keras.io/guides/sequential_model/) to create the model.\n",
    "2. Change number of neurons and layers.\n",
    "3. Play with different activation functions: `relu`,`sigmoid`,`tanh`,`elu`, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pvvhI1k7zHoH",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start creating the model_2\n",
    "\n",
    "\n",
    "\n",
    "# End of creating the model_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8Z6H8IFQ0a5-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Tweak the training process\n",
    "\n",
    "**Hint**: \n",
    "1. Try different `epochs` and `batch_size` is the easiest thing, but the gain might be very little\n",
    "2. Change `loss function` as another strategy, but need more effort\n",
    "3. In addition, there are [many optimizers](https://keras.io/api/optimizers/) you can try, but you should adjust their parameters.\n",
    "4. Shuffle the dataset during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6Zt3E3wtdOl",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start model training\n",
    "\n",
    "\n",
    "\n",
    "# End of model training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zi-z-tDB6y2n",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Plotting loss curve and accuracy curve during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q_GLKlgy7BF9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start loss curve\n",
    "\n",
    "\n",
    "\n",
    "# End of loss curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_BGygsL6mOv",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Model testing and visualization the test result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MSRGvJts7Axb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Start model test\n",
    "\n",
    "\n",
    "\n",
    "# End of model test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VVw_E8GS9pqq",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Save model\n",
    "We finally trained a model with acceptable accuracy, we can port it to the hard disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YfQtmMIU8pCj",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.save('model_logic_function.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KnJ22JVN-0nL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "let's try to load the saved model and check if we saved it correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JgjRrxBY-9ZP",
    "outputId": "07dc4517-9f74-4253-cf8e-1633bf03086c",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "saved_model = keras.models.load_model('model_logic_function.h5')\n",
    "saved_model.summary()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab01.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}