{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcFhSB8lde8B",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 02: Image Recognition\n",
    "\n",
    "In this lab session, we will use the gesture classification task as an example to demonstrate how to process image data with deep learning networks. This lab session includes:\n",
    "- Dataset preparation \n",
    "  - Downloading \n",
    "  - Analysis and visualization \n",
    "  - Data augmentation\n",
    "- CNN model building\n",
    "  - From scratch\n",
    "  - Transfer learning\n",
    "- Training process\n",
    "  - Early Stopping\n",
    "  - Understanding the learning curve\n",
    "  - Layer freezing\n",
    "\n",
    "Open in google colab -> [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg#left)](https://colab.research.google.com/github/SuperChange001/deeplearning_labs/blob/main/Lab02/Lab02.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2EV4SBCecxWZ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set up TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eP98q8pBb0vc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "First we import some libraries for image processing and utilities as well as TensorFlow. Note that the module \"image_dataset_from_directory\" is necessary to download our dataset from Google."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpyxHufwFKYp",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "\n",
    "# Set the seed value for experiment reproducibility.\n",
    "seed = 32\n",
    "tf.random.set_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPfEkbUYG_ci",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import the Gesture dataset\n",
    "\n",
    "Download and extract the `zip` file containing the datasets with `tf.keras.utils.get_file`. \n",
    "\n",
    "*Tip: [Here](https://www.tensorflow.org/datasets/catalog/overview) are more datasets available for you to try.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oy8hBBOCMlA0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Download our dataset used for training\n",
    "TRAIN_SET_URL = 'https://storage.googleapis.com/learning-datasets/rps.zip'\n",
    "path_to_zip = tf.keras.utils.get_file('rps.zip', origin=TRAIN_SET_URL, extract=True, cache_dir='/content')\n",
    "train_dir = os.path.join(os.path.dirname(path_to_zip), \"rps\")\n",
    "\n",
    "# As well as the validation dataset\n",
    "VAL_SET_URL = 'https://storage.googleapis.com/learning-datasets/rps-test-set.zip'\n",
    "path_to_zip2 = tf.keras.utils.get_file('rps-test-set.zip', origin=VAL_SET_URL, extract=True, cache_dir='/content')\n",
    "validation_dir = os.path.join(os.path.dirname(path_to_zip2), \"rps-test-set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HBernrNHwFc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Then we can generate the dataset from the image files in the directory with `tf.data.Dataset`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E0HnHBGmNLh5",
    "outputId": "70321ec4-7166-4d78-ceab-742ab0c9cb9b",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "IMG_SIZE = (96, 96) # why? what is the original image size?\n",
    "\n",
    "train_dataset = image_dataset_from_directory(train_dir,\n",
    "                                             shuffle=True,\n",
    "                                             batch_size=BATCH_SIZE,\n",
    "                                             image_size=IMG_SIZE)\n",
    "\n",
    "validation_dataset = image_dataset_from_directory(validation_dir,\n",
    "                                                  shuffle=True,\n",
    "                                                  batch_size=BATCH_SIZE,\n",
    "                                                  image_size=IMG_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mRHrIWLUJDQR",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lets display some images of our dataset, together with their class names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 571
    },
    "id": "7h7P2R8QNOmw",
    "outputId": "19c64b7e-622f-4e8d-d314-b1d7dc934663",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class_names = train_dataset.class_names\n",
    "num_classes = len(train_dataset.class_names)\n",
    "print(\"Class names:\" , class_names)\n",
    "print(\"Number of classes:\", num_classes)\n",
    "\n",
    "plt.figure(figsize=(9, 9))\n",
    "for images, labels in train_dataset.take(1):\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(images[i].numpy().astype(\"uint8\"))\n",
    "    plt.title(class_names[labels[i]])\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGJ5uDKeJuvN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Split test set and validation set\n",
    "\n",
    "We now take one fifth of the validation data set to use as a test set. The validation data set is used to observe whether overfitting occurred during training, while the test data set is used for the final test after training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kkbBixY8NVbP",
    "outputId": "35fe3495-48a9-4a0c-f626-6501ced7aedb",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "val_batches = tf.data.experimental.cardinality(validation_dataset)\n",
    "\n",
    "test_dataset = validation_dataset.take(val_batches // 5)\n",
    "validation_dataset = validation_dataset.skip(val_batches // 5)\n",
    "\n",
    "print('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\n",
    "print('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_FEDUN8KpFN",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Configure the dataset for performance\n",
    "Use buffered prefetching to load images from the disk without having I/O become blocking. To learn more about this method see the [data performance](https://www.tensorflow.org/guide/data_performance) guide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vsHhFIIBNapg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "train_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\n",
    "test_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcycZDeAR_vU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## The Fist Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VrR8LxQ0K58G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Create a CNN model\n",
    "Let's define a simple Convolutional Neural Network Model(CNN) with several convolutional layers, followed by max pooling layers and a dense layer.\n",
    "\n",
    "*Tips: Information about parameters of [Conv2D](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D) and [Dense](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense) layers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xb9G5IpENiTt",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Create the cnn model \n",
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "model = Sequential([\n",
    "  layers.InputLayer(input_shape=IMG_SHAPE),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),  \n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv-iDxtTMeDo",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Compile the model\n",
    "\n",
    "Compile the model before training it. We can define the used optimizer and the learning rate, the loss function, and which metrics to display while training: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zxUnhYsvNkta",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "InAShFTwNoYE",
    "outputId": "da01b1a0-7f8b-4244-ebb7-51c04d0d0794",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HBJMcnGVMsAA",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Train the model\n",
    "\n",
    "Now we should train the model for 10 epochs and see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qk2aabV0NqlU",
    "outputId": "7faa2c59-34ab-4f8e-80f9-bf68e27e766e",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=10,\n",
    "                    validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tt-URjpJQeGU",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Learning curves\n",
    "\n",
    "Let's take a look at the learning curves of the training and validation accuracy/loss of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "KH2TeeCW4ZdZ",
    "outputId": "0fe9d3e6-2f4d-46ee-b582-5a560a27e930",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define a function so we can reuse it later\n",
    "def draw_learning_curves(history):\n",
    "  acc = history.history['accuracy']\n",
    "  val_acc = history.history['val_accuracy']\n",
    "\n",
    "  loss = history.history['loss']\n",
    "  val_loss = history.history['val_loss']\n",
    "\n",
    "  plt.figure(figsize=(8, 8))\n",
    "  plt.subplot(2, 1, 1)\n",
    "  plt.plot(acc, label='Training Accuracy')\n",
    "  plt.plot(val_acc, label='Validation Accuracy')\n",
    "  plt.legend(loc='lower right')\n",
    "  plt.ylabel('Accuracy')\n",
    "  plt.title('Training and Validation Accuracy')\n",
    "\n",
    "  plt.subplot(2, 1, 2)\n",
    "  plt.plot(loss, label='Training Loss')\n",
    "  plt.plot(val_loss, label='Validation Loss')\n",
    "  plt.legend(loc='upper right')\n",
    "  plt.ylabel('Cross Entropy')\n",
    "  plt.title('Training and Validation Loss')\n",
    "  plt.xlabel('epoch')\n",
    "  plt.show()\n",
    "\n",
    "draw_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1VFtZMGSWG-",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also check the performance of the model against new data by using the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGmv91IF4zuY",
    "outputId": "5c59eea7-3ab9-49b4-a6ae-434f1df01a0a",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ahJ89Ge1S8OL",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Your simple CNN model achieves 100% accuracy on the training set, it is working! **But**, on the validation set and test set, the model doesn't perform as good as a training set, why?\n",
    "\n",
    "*The model fits too well into the training set and then it becomes difficult for the model to generalize to new examples that were not in the training set. Your model recognizes specific images in your training set instead of general patterns, this is called **overfitting**.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjvZhYxuWobV",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Improvement\n",
    "*Some strategies that could to overcome overfitting:*\n",
    "- Increase the size of the training set\n",
    "  - Add more data\n",
    "  - Data augmentation\n",
    "- Add dropout layers\n",
    "- Early stop to avoid overtraining\n",
    "- Take model architectures that generalize well"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eyoFJeh7XxSc",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Data augmentation\n",
    "We want to add some random flips and rotations to the input images to get a more \"varied\" range of inputs. To do this, we define two pre-processing layers that the inputs run through in order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhPlM6lkYgu2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n",
    "  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WXPnVDHlY2DI",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can add [other layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers/experimental/preprocessing), such as randomly zoom in/out.\n",
    "\n",
    "Note: These layers are active only during training, when you call `model.fit`. They are inactive when the model is used in inference mode in `model.evaulate` or `model.predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kpt4XYzGZgY4",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's repeatedly apply these layers to the same image and see the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "xOVZvqOAZm6D",
    "outputId": "1dae51e0-810a-40ef-8f5f-0957afe90b58",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for image, _ in train_dataset.take(1):\n",
    "  plt.figure(figsize=(10, 10))\n",
    "  first_image = image[0]\n",
    "  for i in range(9):\n",
    "    ax = plt.subplot(3, 3, i + 1)\n",
    "    augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n",
    "    plt.imshow(tf.dtypes.cast(augmented_image[0], tf.uint8))\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvU6LXQNbPLQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Embedding data augmentation and dropout in the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aCvKNIWnbOjy",
    "outputId": "4cc307cc-b23a-467a-9176-4bb8502fdd49",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "  layers.InputLayer(input_shape=IMG_SHAPE),\n",
    "  data_augmentation,\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),  \n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(64, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Dropout(0.2),\n",
    "  layers.Flatten(),\n",
    "  layers.Dense(64, activation='relu'),\n",
    "  layers.Dense(num_classes)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "base_learning_rate = 0.001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7TZk_GxfZoJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Model training and learning curve\n",
    "\n",
    "**Note**: This time we train the model with the early-stop strategy. Read more about `tf.keras.callbacks.EarlyStopping` [here](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 947
    },
    "id": "6V3Hvv8ybp_l",
    "outputId": "851234c0-1162-4530-c76e-f6edac5ed5bf",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=100,\n",
    "                    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
    "                    validation_data=validation_dataset)\n",
    "\n",
    "draw_learning_curves(history)\n",
    "\n",
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sKBXOXe2guPQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Transfer learning\n",
    "\n",
    "> This part is adapted from a [tutorial](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb?hl=es) from TensorFlow. You will create the base model from the MobileNet V2 model developed at Google. This is pre-trained on the ImageNet dataset, a large dataset consisting of 1.4M images and 1000 classes. ImageNet is a research training dataset with a wide variety of categories like jackfruit and syringe. This base of knowledge will help us classify gestures from our specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lwyja0mRh_6L",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Rescale pixel values\n",
    "\n",
    "In a moment, you will download tf.keras.applications.MobileNetV2 for use as your base model. This model expects pixel values in [-1, 1], but at this point, the pixel values in your images are in the range of [0, 255]. To adapt to our images we use the preprocessing method included with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL3YBxbHjIQx",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Z3nHHiGjot2",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Create the base model from the pre-trained convnets\n",
    "\n",
    "\n",
    "\n",
    "First, you need to pick which layer of MobileNet V2 you will use for feature extraction. The very last classification layer (on \"top\", as most diagrams of machine learning models go from bottom to top) is not very useful.  Instead, you will follow the common practice to depend on the very last layer before the flatten operation. This layer is called the \"bottleneck layer\". The bottleneck layer features retain more generality as compared to the final/top layer.\n",
    "\n",
    "Now, instantiate a MobileNet V2 model pre-loaded with weights trained on ImageNet. By specifying the include_top=False argument, you load a network that doesn't include the classification layers at the top, which is ideal for feature extraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VE9mdoKtjnTR",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = IMG_SIZE + (3,)\n",
    "base_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                                               include_top=False,\n",
    "                                               weights='imagenet',\n",
    "                                               alpha=0.35)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ii64KyrWkROP",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's see what it does to an example batch of images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VO6jX7wskSQO",
    "outputId": "f4c44d23-c4bd-449a-ffe1-2471938fb532",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataset))\n",
    "feature_batch = base_model(image_batch)\n",
    "\n",
    "print(feature_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akEz-kJzbNcs",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The output information indicates that the feature extractor converts each 90x90x3 image into a 3x3x1280 block of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehtTTPpTkwft",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Freeze the convolutional base\n",
    " It is important to freeze the convolutional base before you compile and train the model. Freezing (by setting `layer.trainable = False`) prevents the weights in a given layer from being updated during training. MobileNet V2 has many layers, so setting the entire model's trainable flag to False will freeze all of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0q7Df2OOkv3R",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.trainable = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l8ObrS84lIkx",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look at the base model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "63XdzGXklJCo",
    "outputId": "180a8e94-49c9-433b-aeff-533aaa95e3e0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wK7L1uFblXll",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Add a classification head\n",
    "To generate predictions from the block of features, average over the spatial 3x3 spatial locations, using a `tf.keras.layers.GlobalAveragePooling2D` layer to convert the features to  a single 1280-element vector per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "okhoOB4vli39",
    "outputId": "82788aed-1d9c-4529-a7c5-21a662d20bc9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\n",
    "feature_batch_average = global_average_layer(feature_batch)\n",
    "print(feature_batch_average.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HPA_XZBlkxQ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Apply a `tf.keras.layers.Dense` layer to convert these features into a single prediction per image. You don't need an activation function here because this prediction will be treated as a `logit`, or a raw prediction value.  Positive numbers predict class 1, negative numbers predict class 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8KLlMNvlvay",
    "outputId": "a0f5ca7a-5b2c-4c84-afd9-372032105528",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "prediction_layer = tf.keras.layers.Dense(3)\n",
    "prediction_batch = prediction_layer(feature_batch_average)\n",
    "print(prediction_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5tM28OUllyzd",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "####  Build and Compile the stacked model\n",
    "\n",
    "Build a model by chaining together the data augmentation, rescaling, base_model and feature extractor layers using the [Keras Functional API](https://www.tensorflow.org/guide/keras/functional). As previously mentioned, use `training=False` as our model contains a BatchNormalization layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6DcfJ25mGxh",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "inputs = tf.keras.Input(shape=IMG_SHAPE)\n",
    "x = data_augmentation(inputs)\n",
    "x = preprocess_input(inputs)\n",
    "x = base_model(x, training=False)\n",
    "x = global_average_layer(x)\n",
    "x = tf.keras.layers.Dropout(0.2)(x)\n",
    "outputs = prediction_layer(x)\n",
    "model = tf.keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BQ84fNktmKfi",
    "outputId": "310a1363-e47f-4479-e6cb-5f534b6c564f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_learning_rate = 0.0001\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=base_learning_rate),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXpIfEShmn_G",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The 0.4M parameters in MobileNet are frozen, but there are 3.8K trainable parameters in the Dense layer.  These are divided between two tf.Variable objects, the weights and biases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C4MMWrLHnQkG",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Train the model\n",
    "After training for 10 epochs, you should see ~94% accuracy on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yx7-4737nTDp",
    "outputId": "09465722-df1a-42c6-db81-efdc4cdc2e58",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "initial_epochs = 10\n",
    "\n",
    "loss0, accuracy0 = model.evaluate(validation_dataset)\n",
    "\n",
    "print(\"initial loss: {:.2f}\".format(loss0))\n",
    "print(\"initial accuracy: {:.2f}\".format(accuracy0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "id": "IcvQhCEfng-H",
    "outputId": "2c95d3ff-13a9-49d8-b2f2-13665e350595",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_dataset,\n",
    "                    epochs=initial_epochs,\n",
    "                    validation_data=validation_dataset)\n",
    "\n",
    "draw_learning_curves(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wl5F3JDvpQPu",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Fine tuning\n",
    "Now we also try to train some layers in the base model to improve the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJOnpnSGpbPF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Let's take a look to see how many layers are in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OgOWeLmKpP4V",
    "outputId": "2c526ac6-7b1a-44b1-805c-25243fe684f5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of layers in the base model: \", len(base_model.layers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3yIKHBpqBpw",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ok, we can fine-tune the weights in the 54 layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v1rw59XpnrU0",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "base_model.trainable = True\n",
    "\n",
    "# Fine-tune from this layer onwards\n",
    "fine_tune_at = 100\n",
    "\n",
    "# Freeze all the layers before the `fine_tune_at` layer\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "  layer.trainable =  False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ubbKuu_HqRMy",
    "outputId": "5f358f16-5ccb-4231-d01f-6b787ba1d4ef",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              optimizer = tf.keras.optimizers.RMSprop(learning_rate=base_learning_rate/10),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TGb6kaTnqhKF",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Continue training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "41iaovc8qpyf",
    "outputId": "d3be2eb5-515c-4bb8-cebb-72dd335f49cc",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fine_tune_epochs = 10\n",
    "total_epochs =  initial_epochs + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(train_dataset,\n",
    "                         epochs=total_epochs,\n",
    "                         initial_epoch=history.epoch[-1],\n",
    "                         validation_data=validation_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 513
    },
    "id": "djSp7yM_rL1g",
    "outputId": "612b6f72-8fe9-44cf-b00f-b1be693f38c1",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy'] + history_fine.history['accuracy']\n",
    "val_acc = history.history['val_accuracy'] + history_fine.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss'] + history_fine.history['loss']\n",
    "val_loss = history.history['val_loss'] + history_fine.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.ylim([0.8, 1])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "          plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.ylim([0, 1.0])\n",
    "plt.plot([initial_epochs-1,initial_epochs-1],\n",
    "         plt.ylim(), label='Start Fine Tuning')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24kVj4njr0cy",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " ### Evaluation and prediction\n",
    " Finaly you can verify the performance of the model on new data using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bU-fLltPr4C_",
    "outputId": "935c5925-97cb-432e-c284-28c24e7843fa",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_dataset)\n",
    "print('Test accuracy :', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLz9Q-nTr6dJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    " And now you are all set to use this model to predict the input gesture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 660
    },
    "id": "p6ZjV_NisHm2",
    "outputId": "d82d400c-ef47-471c-aeab-ee8f7273fb3d",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#Retrieve a batch of images from the test set\n",
    "image_batch, label_batch = test_dataset.as_numpy_iterator().next()\n",
    "predictions = model.predict_on_batch(image_batch)\n",
    "\n",
    "#Get predictions\n",
    "predList = []\n",
    "for pred in predictions:\n",
    "  predList.append(np.argmax(pred))\n",
    "\n",
    "predictions = np.asarray(predList)\n",
    "\n",
    "print('Predictions:\\n', predictions)\n",
    "print('Labels:\\n', label_batch)\n",
    "\n",
    "#Draw fist 10 gestures\n",
    "plt.figure(figsize=(10, 10))\n",
    "for i in range(9):\n",
    "  ax = plt.subplot(3, 3, i + 1)\n",
    "  plt.imshow(image_batch[i].astype(\"uint8\"))\n",
    "  plt.title(class_names[predictions[i]])\n",
    "  plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wubiYgGzQQtk",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## What is next?\n",
    "\n",
    "1. Train a CNN on another dataset\n",
    "    ```\n",
    "    Dog and cat:\n",
    "    https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \n",
    "    Horse and human:\n",
    "    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
    "\n",
    "    ```\n",
    "2. Apply all strategies introduced in this notebook to train the CNN\n",
    "3. Your can save your well-trained gesture model, which you can use in the next Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zymRXfyOQ7rM",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# dataset loading and preparing, try more augmentation methods\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Creating your CNN model(either from scratch or transfer learning)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Train the CNN model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test the trained CNN model and \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store it to the hard disk\n",
    "model.save('/content/model_rps.h5')\n",
    "\n",
    "from google.colab import files\n",
    "files.download('/content/model_rps.h5') "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Lab02.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3.10.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "8a94588eda9d64d9e9a351ab8144e55b1fabf5113b54e67dd26a8c27df0381b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
